\section{Implementation}

{\bf Kartik's Note: This section is a placeholder. Contents moved to respective hypervisor/container sections.}

\subsection{Hypervisor Replacement} 
The prototype of \arch's hypervisor replacement is developed using KVM/QEMU virtualization platforms with Linux kernel version 4.13.0 and QEMU version 2.9.0. KVM is a linux kernel module which is responsible for core hypervisor functionalities. QEMU is a user space process that communicates with VMs and is responsible to handle I/O operations of VMs. The VM runs unmodified Linux kernel version 4.13.0 and uses para-virtual drivers for I/O operations.

Our solution is based on QEMU's live migration. Differently, \arch's hypervisor replacement operation only involves the transfer of VCPU and I/O state of the VM. Before invoking the replacement operation, memory is already mapped into the address space of the new hypervisor (i.e., out of the critical path of hypervisor replacement). Therefore, we modified the QEMU code to disable the dirty page tracking and transfer of pages to the destination VM. We also disable the transfer of dirty pages during the stop-and-copy phase of live migration. The VCPUs are paused only during the stop-and-copy phase which results in very little hypervisor replacement/refresh time.

\para{Memory Relocation.}
The source VM's QEMU makes an IOCTL to request current KVM hypervisor to pin the VM pages in memory during the boot time. Hypervisor uses memory slots to get the VM frame numbers (GFNs) of the VM and translates all the VM GFNs to hypervisor GFNs. KVM then transfers the VM GFNs and hypervisor GFNs mapping information to the hyperplexor using \texttt{hypercall\_set} hypercalls. The hyperplexor pins the pages in memory using the hypervisor GFNs. Hyperplexor creates the \arch table with VM GFN to physical frame number (PFNs) mappings and uses a VM ID to indicate the owner of the \arch table. 

When the current hypervisor has to be replaced with updated hypervisor, the destination VM makes an IOCTL to the new KVM hypervisor to map the VM pages to the PFNs of the source VM using the \arch table. The new hypervisor also iterates through the memory slots to get all the VM GFNs. KVM then gets the free hypervisor GFNs to map the VM GFNs. To ensure that the hypervisor GFNs are clear of any previous mappings, we flush the entries in TLB and page table of QEMU. The VM GFNs and hypervisor GFNs are transferred to hyperplexor through \texttt{hypercall\_map} hypercalls. The hyperplexor maps the destination VM GFNs to the PFNs using the \arch table and adds the destination VM to the owner of the \arch table..

When the hypervisor replacement operation is invoked, the source VM requests hyperplexor during the stop-and-copy phase to change the ownership of the VM pages to the destination VM. The old hypervisor unmaps the VM pages from its address space and is gracefully shutdown. 

\para{Optimization.}
A hypercall is costly as it involves a switch from the non-root mode to the root mode with a VMExit. For each VMExit, the hypervisor has to store the VMExit information, save the state of the VM and load the host state information (vice-versa on VMEntry). To minimize the number of hypercalls, we write the VM GFN to hypervisor GFN mappings to pages allocated in hypervisor and transfer the address of the pages to the hyperplexor. The number of hypercalls reduce to the number of pages required to store the memory mappings information. The \arch table is implemented as a hash table for fast lookup of the VM GFN to PFN mapping.

\subsection{OS Replacement}
We have also implemented an OS replacement prototype based on CRIU \cite{criu} --- a checkpoint/restore tool implemented in the user space. CRIU consists of two stages: checkpointing (on the source VM) and restoration (on the destination VM). By automating these two parts, we implemented the live OS replacement for containers. \arch's OS replacement operation involve the transfer of all the state of containers and their processes (e.g., file descriptors, memory maps, registers, namespaces, cgroups, etc.) except for memory content from the source VM (with the stale OS) to the destination VM (with the replacement OS).

In our implementation, when the OS replacement operation is invoked, one of the target processes is paused. During the checkpointing phase, we collect all state of the process except for its memory content and build the \arch table. During the restoration phase, we restore the process state on the destination VM and relocate the memory ownership. In the final stage, we resume running the process on the destination VM. The source VM unmaps the pages of the process from its address space. We repeat this process until all target processes are moved to the destination VM. Then the source VM can be gracefully shutdown. 

\para{Checkpointing.}
We  modified CRIU's checkpointing code to replace the procedure of dumping memory content with the one of building the \arch table by calling \arch's provided system call, \texttt{syscall\_set}. The function takes an array of GVAs and the their mapped GPAs. To obtain such GVA-to-GPA page mappings, we exploit CRIU's existing mechanism of reading the \texttt{/proc/[PID]/pagemap} file, which stores the GVA-to-GPA mappings. 
We further use the \texttt{memalign} function to allocate memory for the input arrays, which is page size aligned. The \texttt{syscall\_set} system call translates the base address of the two arrays (containing GVAs and GPAs) into GPAs, and invokes the \texttt{hypercall\_set} hypercall by transferring the GPAs of the arrays to the hyperplexor.  As stated above, the \texttt{hypercall\_set} hypercall locates (or creates) the \arch table for the process (by PID), obtains the GVA-to-PA mappings, and insert them to the \arch table.

\para{Restoration.} On the destination VM, we modify CRIU's restoration code to replace the procedure of loading memory content with the one of relocating address space via \arch's new system call, \texttt{syscall\_map}. Before invoking  \texttt{syscall\_map}, the restorer needs to restore GVAs (i.e., received from the source VM) using the \texttt{mmap} system call. It then invokes \arch's \texttt{syscall\_map} by passing an array of GVAs. The \texttt{syscall\_map} system call in turn gets free VM memory pages, GPAs, and establish the GVA-to-GPA page mappings in the process's page table. With such GVA-to-GPA page mappings, the \texttt{syscall\_set} system call invokes the \texttt{hypercall\_mpa} hypercall by transferring such mappings to the hyperplexor. As stated above, the \texttt{hypercall\_map} hypercall installs the GPA-to-PA page mappings in the destination VM's EPT table.

%\para{Hypervisor Replacement}
%The hypervisor replacement operation only involves the transfer of vCPU and I/O state of the guest. Before invoking the replacement operation, memory is already mapped into the address space of the new hypervisor. The default QEMU live migration code keeps track of the dirty pages using a bitmap. The bitmap is synchronized after every pre-copy iteration round. We modify the QEMU code to disable the dirty page tracking and transfer of pages to the destination guest. We also disable the transfer of dirty pages during the stop and copy phase of live migration. The vCPUs are paused only during the stop and copy phase which results in very less hypervisor replacement/refresh time. 
