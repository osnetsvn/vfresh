\section{Implementation}
The prototype is developed using KVM/QEMU virtualization platforms with Linux kernel version 4.13.0 and QEMU version 2.9.0. KVM is a linux kernel module which is responsible for core hypervisor functionalities. QEMU is a user space process that communicates with guest and is responsible to handle I/O operations of guest. The guest runs unmodified Linux kernel version 4.13.0 and uses para-virtual drivers for I/O operations.

\subsection{Memory Mappings Transfer}
The source guest QEMU makes an IOCTL to request current KVM hypervisor to pin the guest pages in memory during the boot time. Hypervisor uses memory slots to get the guest frame numbers (GFNs) of the guest and it translates all the guest GFNs to hypervisor GFNs. KVM then transfers the guest GFNs and hypervisor GFNs mapping information to the hyperplexor using hypercalls. The hyperplexor pins the pages in memory using the hypervisor GFNs. Hyperplexor then creates a grant table with guest GFN to physical frame number (PFNs) mappings and maintains an additional field in grant table to indicate the current owner of the guest pages. 

When the current hypervisor has to be replaced with updated hypervisor, the destination guest makes an IOCTL to the new KVM hypervisor to map the guest pages to the PFNs of the source guest using grant table. The new hypervisor also iterates through the memory slots to get all the guest GFNs. KVM then gets the free hypervisor GFNs to map the guest GFNs. To ensure that the hypervisor GFNs are clear of any previous mappings, we flush the entries in TLB and page table of QEMU. The guest GFNs and hypervisor GFNs are tranferred to hyperplexor through hypercalls. The hyperplexor maps the destination guest GFNs to the PFNs using grant table and adds the destination guest to the ownership field.

When the hypervisor replacement operation is invoked, the source guest requests hyperplexor during stop and copy phase to change the ownership of the guest pages to the destination guest. The hyperplexor updates the grant table with the current owner of the guest pages. The old hypervisor unmaps the guest pages from it's address space and is gracefully shutdown. 

\textbf{Optimization}
Ideally, the number of hypercalls from hypervisor to hyperplexor is equal to the number of guest GFNs to be pinned or mapped to PFNs. The number of hypercalls increase with the size of the guest. Hypercalls are costly as it involves a switch from non-root mode to root mode. For each VMExit, the hypervisor has to store the VMExit information, save the state of the VM and load the host state information and vice-versa on VMEntry. To minimize the number of hypercalls, we write the guest GFN to hypervisor GFN mappings to pages allocated in hypervisor and transfer the address of the pages to the hyperplexor. The number of hypercalls reduce to the number of pages required to store the memory mappings information. The grant table is implemented as a hash table for fast lookup of guest GFN to PFN mapping.

\subsection{SR-IOV and PCI Passthrough}
to be taken from design section

\subsection{Hypervisor Replacement}
The hypervisor replacement operation only involves the transfer of VCPU and I/O state of the guest. Before invoking the replacement operation, memory is already mapped into the address space of the new hypervisor. The default QEMU live migration code keeps track of the dirty pages using a bitmap. The bitmap is synchronized after every pre-copy iteration round. We modify the QEMU code to disable the dirty page tracking and transfer of pages to the destination guest. We also disable the transfer of dirty pages during the stop and copy phase of live migration. The VCPUs are paused only during the stop and copy phase which results in very less hypervisor replacement/refresh time. 
