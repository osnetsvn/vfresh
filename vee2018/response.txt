
Thanks for your constructive feedback and suggestions. Please see our responses below.

Review #40A
Q1: Domain-specific unikernel, portability to ARMv8, and open source.
R1: Thanks for these suggestions for further investigation. The 90MB hyperplexor size is from configuring a minimal Linux without unnecessary components, and could be further reduced. We will provide a link for our current implementation. We are working on porting the code to newer Linux and QEMU and would like to make it open source in the future.    

Q2: Purpose of dedicated cores.
R2: Though not essential to VFresh, pinning VCPUs to dedicated cores reduces contention between hyperplexor and the hypervisor. We will clarify this in our final submission.

Q3: Any other hypervisor state that would be problematic after a replacement?
R3: Our prototype is able to correctly relocate all VM state (memory maps, VCPU and I/O), just as standard live migration does. However, its possible that if the old hypervisor maintains any atypical state about its VMs, such as for VM introspection or other stateful hypervisor-level services, those would need to be serialized and transferred as well on a case-by-case basis.
 
Review #40B
Q4: Slowdown when new VMs are resumed in the new hypervisor.
R4: We'll include more overhead measurements in our final version. We're also investigating ways to populate target EPT/shadow EPT entries ahead of time, out of the critical path of hypervisor replacement. 

Q5: The inherent overhead of nested virtualization.
R5:  Admittedly, nesting incurs additional overheads, reducing which is an important broader problem beyond just for VFresh. We focus on minimizing some of these overheads, especially I/O-related. Our evaluation shows very close performance between nested and non-nested cases for network-intensive workloads. Ongoing optimization efforts (e.g., NEVE SOSP'17) and future hardware extensions could further reduce nesting overheads and benefit VFresh as well.

Q6: Identify and package all the VM-related state?
R6: Please refer to R3.  
 
Reviewer #40C:
Q8: Reliance on nested virtualization and limitations of VFresh.
R8: Please refer to R5. We will add more discussions on tradeoffs and overheads.

Q9:Comments on Section 2
R9: We will include a background of KVM/QEMU, and also larger VM sizes for the motivational experiment.

Q10: Comments on Section 3.
R10: Thanks for your insightful observations. We will better explain these. Indeed, Shadow EPT transfer is non-trivial without intrusive changes to the L1 hypervisor, since it reflects the combined states from both L1 and L0. For IPIs, we are investigating direct delivery of L1 IPIs using posted-interrupt support and/or by adapting techniques from DID (VEE'15) and ELI (VEE'12). IPIs between L2 VCPUs may still require VMExits. 

Q11: Comments on Section 5.
R11: We will address these as well. We didn't avoid stop and copy (sorry for the confusion). Rather, we completely eliminate memory copying, including during the stop-and-copy phase, which now consists of only VCPU and I/O state transfers. The working set size in section 5.1.2 is 1GB, where repeated Quicksorts work on a different 200KB memory regions.
